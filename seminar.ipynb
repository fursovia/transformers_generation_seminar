{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cc3233",
   "metadata": {},
   "source": [
    "# Tinkoff Seminar\n",
    "## LM, Seq2seq\n",
    "\n",
    "На этом семинаре разберем с вами\n",
    "\n",
    "1. Процесс декодирования в генеративных авторегрессионных моделях\n",
    "2. Как работать с моделями huggingface.transformers\n",
    "3. Promt engineering\n",
    "4. Обучение и инференс GPT2\n",
    "5. Инференс MT моделей\n",
    "\n",
    "Семинар создан [@fursov](https://t.me/fursov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d957dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -i 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762800dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# если у вас несколько ГПУ на выбор, можете выбрать одну из них с помощью переменной окружения\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c620ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2c17f",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "В этом разделе разберемся, как работать с GPT2 в библиотеке transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13669f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sberbank-ai/rugpt3medium_based_on_gpt2'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e920a0",
   "metadata": {},
   "source": [
    "### Работа с токенизатором в transformers\n",
    "\n",
    "Как из текста сделать вход для модели?\n",
    "\n",
    "GPT2 требует на вход \n",
    "\n",
    "1. input_ids -- это айдишники токенов\n",
    "2. attention_mask -- это указатель, куда мы можем делать атеншн (мы не хотим делать атеншн на специальные токены)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f440ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'всем привет'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03995",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оказывается, перед нами не dict!\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8869ff58",
   "metadata": {},
   "source": [
    "Мы можем посмотреть, какие интересные методы есть у этого объекта в [документации](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.to)\n",
    "\n",
    "\n",
    "Оказывается, есть метод `.to(device)`, который сильно упрощает жизнь!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21860cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мы хотим получить торч-тензоры\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19560aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c88c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, num_tokens]\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee438ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 учится без падингов. все тексты конкатинируются вместе\n",
    "\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35634e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# но есть специальный токен конца текста\n",
    "\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd862b0",
   "metadata": {},
   "source": [
    "С помощью токенайзера мы можем, как энкодить, так и декодить последовательность айдишников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16061de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([ 275, 1515, 6129, 123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b10d99",
   "metadata": {},
   "source": [
    "### Процесс инференса GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace операция!\n",
    "\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5191fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# действительно\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вызываем .forward\n",
    "\n",
    "out = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24345473",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bab599",
   "metadata": {},
   "source": [
    "По логитам можем предсказать следующее слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a32e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем наиболее вероятный токен\n",
    "\n",
    "next_token_id = out['logits'][:, -1, :].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa520f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ids = inputs['input_ids'].cpu().numpy().tolist()[0] + [next_token_id.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605ba60",
   "metadata": {},
   "source": [
    "Модель решила поставить знак ударения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(sentence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfa94f",
   "metadata": {},
   "source": [
    "Соответственно, мы можем повторить операцию и получить следующий наиболее вероятный токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c299b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.tensor([sentence_ids], device=device))\n",
    "next_token_id = out['logits'][:, -1, :].argmax()\n",
    "sentence_ids = sentence_ids + [next_token_id.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(sentence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b9b4c",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Давайте попробуем сами реализовать top-k, top-p сэмплирвование с температурой. Но для начала попробуем просэмплировать без top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48048d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('всем привет', return_tensors='pt').to(device)\n",
    "input_ids_list = inputs['input_ids'][0].cpu().numpy().tolist()\n",
    "\n",
    "out = model(**inputs)\n",
    "logits = out.logits\n",
    "next_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(next_token_logits, num_samples=10):\n",
    "    # получаем вероятности с помощью софтмакса\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    next_tokens = torch.multinomial(probs, num_samples=num_samples, replacement=True)\n",
    "\n",
    "    for next_token in next_tokens[0]:\n",
    "        decoded = tokenizer.decode(input_ids_list + [next_token.item()])\n",
    "        print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сэмплируем из мультиноминального распределения\n",
    "\n",
    "sample_from_logits(next_token_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf30230",
   "metadata": {},
   "source": [
    "Примеры получились более-менее. А что если добавить температуру?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ce7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 2.0\n",
    "\n",
    "sample_from_logits(next_token_logits / temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452a99e",
   "metadata": {},
   "source": [
    "Температура оказалось слишком высокой, так как некоторые примеры странные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5\n",
    "\n",
    "sample_from_logits(next_token_logits / temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.1\n",
    "\n",
    "sample_from_logits(next_token_logits / temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a258ca",
   "metadata": {},
   "source": [
    "Температура ниже 0.5 ведет к более консервативным генерациям. Пробуем занулить вероятности токенов с помощью top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "\n",
    "def apply_top_k(scores, top_k=top_k):\n",
    "    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
    "    scores = scores.masked_fill(indices_to_remove, -float(\"Inf\"))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403477f2",
   "metadata": {},
   "source": [
    "Мы получаем всего 5 уникальных примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_logits(\n",
    "    apply_top_k(next_token_logits), num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5e452",
   "metadata": {},
   "source": [
    "Соответственно, мы теперь можем даже комбинировать top-k и температуру. Подбирать гиперпараметры декодирования нужно очень внимательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998afb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_logits(\n",
    "    apply_top_k(next_token_logits / 2.0, top_k=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9355",
   "metadata": {},
   "source": [
    "Хотя температура высокая, но top-k не дает генерировать совсем неправдоподобные варианты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3bf3f",
   "metadata": {},
   "source": [
    "## GenerationMixin\n",
    "\n",
    "Все генеративные модели в библиотеке **transformers** имеют метод generate, что упрощает нам генерацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aafd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60229a7a",
   "metadata": {},
   "source": [
    "Для понимания, советую включать **return_dict_in_generate=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f393a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.generate(**inputs, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38a65b",
   "metadata": {},
   "source": [
    "Теперь вместо последовательности айдишников мы по типу объекта можем понять, какой алгоритм декодирования был применен.\n",
    "\n",
    "Соответственно, можно поиграть со всеми основными типами декодирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# бим серч\n",
    "\n",
    "decoded = model.generate(**inputs, num_beams=10, return_dict_in_generate=True)\n",
    "print(type(decoded))\n",
    "tokenizer.decode(decoded.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c87e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сэмплирование\n",
    "\n",
    "decoded = model.generate(**inputs, do_sample=True, return_dict_in_generate=True)\n",
    "print(type(decoded))\n",
    "tokenizer.decode(decoded.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74857e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сэмплирование c температурой\n",
    "\n",
    "decoded = model.generate(**inputs, do_sample=True, temperature=0.5, return_dict_in_generate=True)\n",
    "print(type(decoded))\n",
    "tokenizer.decode(decoded.sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc7165",
   "metadata": {},
   "source": [
    "### Проблемы с повторами\n",
    "\n",
    "Что делать если модель начала повторяться?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'всем привет всем привет всем привет всем привет всем привет всем привет всем привет'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель продолжает генерировать \"всем привет\"\n",
    "generated = model.generate(**inputs)\n",
    "tokenizer.decode(generated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мы можем перевзвесить вероятности для токенов, которые мы уже видели\n",
    "# идея отсюда: https://arxiv.org/pdf/1909.05858.pdf\n",
    "\n",
    "generated = model.generate(**inputs, repetition_penalty=4.0)\n",
    "tokenizer.decode(generated[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626a85b",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "GPT2 модель достаточно сильная даже без дообучения, чтобы решать разные задачи\n",
    "\n",
    "### Рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0211043",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Самый лучший фильм это —', return_tensors='pt')\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3745c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.generate(**inputs, num_beams=4)\n",
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель может посоветовать фильм, который стоит посмотреть\n",
    "\n",
    "generated = model.generate(**inputs, do_sample=True, top_p=0.7, num_return_sequences=4)\n",
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a1ac2",
   "metadata": {},
   "source": [
    "### Диалоговый агент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"— Привет\n",
    "— Здраствуй\n",
    "— Как дела?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20804f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# модель умеет продолжать диаплог\n",
    "\n",
    "generated = model.generate(**inputs, do_sample=True, top_p=1.7, num_return_sequences=4)\n",
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebff36",
   "metadata": {},
   "source": [
    "### [Your idea here]\n",
    "\n",
    "Попробуйте придумать, где еще можно применить GPT!\n",
    "\n",
    "Вдохновиться можно на примерах отсюда https://github.com/elyase/awesome-gpt3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad71955",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Если вам понадобиться дообучить GPT2 под свои задачи (например, в Тинькофф можно дообучить на данных общения операторов с клиентами), то лучшим способом будет воспользоваться скриптами из huggingface: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
    "\n",
    "\n",
    "А если вы вдруг заходите затюнить очень большую модель, то лучший способ распараллелить обучение у NVIDIA: https://github.com/NVIDIA/Megatron-LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da26c0",
   "metadata": {},
   "source": [
    "# Обучаем GPT2 генерировать стихи\n",
    "\n",
    "Мы собрали датасет четверостиший. Попробуем на нем обучить GPT2 генерировать стихи. Нам из датасета нужно сделать файл, на котором мы сможем обучиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e585de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ace6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('data/data.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4143103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23458d",
   "metadata": {},
   "source": [
    "Нам нужно подготовить .txt файл, где на каждой строчке будет стих. Для улучшения качества обучения мы каждую строку будем разделять специальным токеном **@@LINE_i@@**, чтобы модель различала строки между собой и могла проще предлагать рифмы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc624a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt = []\n",
    "\n",
    "for example in data:\n",
    "    a, b, c, d = example\n",
    "    example_txt = f'@@LINE_0@@ {a} @@LINE_1@@ {b} @@LINE_2@@ {c} @@LINE_3@@ {d}'\n",
    "    data_txt.append(example_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92956b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data.txt', 'w') as w:\n",
    "    for ex in data_txt:\n",
    "        w.write(f'{ex}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab2fc6",
   "metadata": {},
   "source": [
    "Мы хотим, чтобы токенизатор знал о новых специальных токенов, поэтому добавим их в словарь. (Во время обучения **в матрицу эмбедингов** мы также добавим эти новые токены)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1876d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/new_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_tokens(['@@LINE_0@@', '@@LINE_1@@', '@@LINE_2@@', '@@LINE_3@@'], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('data/new_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545be26",
   "metadata": {},
   "source": [
    "### Скрипт обучения на одной gpu выглядит следующим образом\n",
    "\n",
    "(Пробуем запускать в терминале)\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=1 python train_gpt.py \\\n",
    "  --model_name_or_path \"sberbank-ai/rugpt3large_based_on_gpt2\" \\\n",
    "  --tokenizer_name \"data/new_tokenizer\" \\\n",
    "  --model_type \"gpt2\" \\\n",
    "  --train_file \"data/data.txt\" \\\n",
    "  --validation_file \"data/data.txt\" \\\n",
    "  --cache_dir \"data/cache\" \\\n",
    "  --output_dir \"poem_logs\" \\\n",
    "  --per_device_train_batch_size 3 \\\n",
    "  --per_device_eval_batch_size 3 \\\n",
    "  --save_total_limit 3 \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --preprocessing_num_workers 4 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --block_size 512\n",
    "```\n",
    "\n",
    "После обучения пробуем сгенерировать стихи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls logs/checkpoint-340000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'logs/checkpoint-340000/'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_dir).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6db87",
   "metadata": {},
   "source": [
    "## Процесс генерации\n",
    "\n",
    "Теперь мы можем подавать на вход модели, например, первую строку стиха (или первые две) и смотреть, какое продолжение выдаст модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b605aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_from_indexes(indexes):\n",
    "    decoded = tokenizer.decode(indexes[0]).split('@@')\n",
    "    for num, i in enumerate([2, 4, 6, 8]):\n",
    "        try:\n",
    "            line = decoded[i].strip()\n",
    "            if line:\n",
    "                print(line)\n",
    "            else:\n",
    "                print(f'[Line {num + 1} is not generated yet]')\n",
    "        except IndexError:\n",
    "            print(f'[Line {num + 1} is not generated yet]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da17a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запомним айдишники специальных токенов\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "id0, id1, id2, id3 = vocab['@@LINE_0@@'], vocab['@@LINE_1@@'], vocab['@@LINE_2@@'], vocab['@@LINE_3@@']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('@@LINE_0@@ У лукоморья дуб зелёный @@LINE_1@@', return_tensors='pt')\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec322a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# останавливаем генерацию, как только увидим @@LINE_2@@\n",
    "\n",
    "generated = model.generate(**inputs, num_beams=5, eos_token_id=id2)\n",
    "\n",
    "print_from_indexes(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем рандома\n",
    "generated = model.generate(**inputs, do_sample=True, temperature=1.1, eos_token_id=id2)\n",
    "\n",
    "print_from_indexes(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерим полный стих\n",
    "generated = model.generate(**inputs, do_sample=True, temperature=1.1, eos_token_id=id0, max_length=100)\n",
    "\n",
    "print_from_indexes(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fc28d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# пробуем другую затравку\n",
    "\n",
    "inputs = tokenizer('@@LINE_0@@ Тинькофф заходит в дом @@LINE_1@@', return_tensors='pt')\n",
    "inputs.to(device)\n",
    "generated = model.generate(**inputs, num_beams=5, eos_token_id=id0, repetition_penalty=4.0, max_length=100)\n",
    "\n",
    "print_from_indexes(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e2ab4",
   "metadata": {},
   "source": [
    "# Seq2seq на примере MT\n",
    "\n",
    "Работа с Seq2seq моделями в `transformers` очень похожа на работу с LM моделями. Давайте попробуем с помощью opensource модели с https://huggingface.co/models перевести какой-нибудь текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be399b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0991f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MarianTokenizer.from_pretrained(MT_MODEL_NAME)\n",
    "model = MarianMTModel.from_pretrained(MT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82953748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# перед нами типичная transformer-like encoder-decoder архитектура\n",
    "\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728357e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_texts = [\n",
    "    \"I'm gonna make him an offer he can't refuse.\",\n",
    "    \"Here's looking at you, kid.\",\n",
    "    \"Frankly, my dear, I don't give a damn.\",\n",
    "    \"Why so serious?\",\n",
    "    \"Say hello to my li’l friend!\",\n",
    "    \"Life is like a box of chocolates\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6795be5",
   "metadata": {},
   "source": [
    "В этот раз мы будем работать сразу с батчом текстов. Поэтому нам нужно включить `padding` (увеличение длины последовательности с помощью pad token), `truncation` (обрезание длины текста до максимально допустимой длины)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text_inputs = tokenizer(\n",
    "    en_texts,\n",
    "    max_length=256,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "batch_text_inputs = batch_text_inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb45183",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# точно такой же метод для генерации!\n",
    "# хорошей идеей будет всегда включать beam search\n",
    "\n",
    "output = model.generate(**batch_text_inputs, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d71741",
   "metadata": {},
   "source": [
    "Теперь мы можем посчитать **BLEU** скор, приняв перевод из гугл-переводчика за правильный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf33643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_translations = [\n",
    "    \"Я сделаю ему предложение, от которого он не сможет отказаться.\",\n",
    "    \"Тут присматривают за тобой, дитя.\",\n",
    "    \"Честно говоря, моя дорогая, мне наплевать.\",\n",
    "    \"Почему ты такой серьезный?\",\n",
    "    \"Передай привет моему маленькому другу!\",\n",
    "    \"Жизнь похожа на коробку конфет\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus = []\n",
    "\n",
    "for ru_true, ru_pred in zip(google_translations, translations):\n",
    "    bleu = sentence_bleu([ru_pred.lower()], ru_true.lower())\n",
    "    bleus.append(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e971a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Судя по этой метрике, качество перевода не очень хорошее, хотя перевод модели мне нравится\n",
    "\n",
    "mean_bleu = sum(bleus) / len(bleus)\n",
    "print(f'BLEU = {mean_bleu:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
